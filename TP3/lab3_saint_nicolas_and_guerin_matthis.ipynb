{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: A. Gramfort, H. Janati, M. Massias\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal coordinate descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 22nd of november at noon**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab3_saint_nicolas_and_guerin_matthis.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"nicolas\"\n",
    "ln1 = \"saint\"\n",
    "fn2 = \"matthis\"\n",
    "ln2 = \"guerin\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(),\n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5938/428856815.py:4: DeprecationWarning: Please use `toeplitz` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import toeplitz\n"
     ]
    }
   ],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b> We ask you to code\n",
    "     <ul>\n",
    "         <li>cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$</li>\n",
    "         <li>greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "\n",
    "    x_hat = np.zeros(n_features)\n",
    "    residuals = b - A.dot(x_hat)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        i = t % n_features\n",
    "        old_x_i = x_hat[i]\n",
    "        x_hat[i] += A[:, i].dot(residuals) / np.sum(A[:, i] ** 2)\n",
    "        residuals += A[:, i] * (old_x_i - x_hat[i])\n",
    "\n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(np.sum(residuals ** 2) / 2.)\n",
    "\n",
    "    return x_hat, np.array(all_objs)\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis=0) ** 2\n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update:\n",
    "        j = np.argmax(np.abs(gradient))\n",
    "        old_w_j = w[j]\n",
    "        w[j] -= gradient[j] / lips_const[j]\n",
    "        # update gradient:\n",
    "        gradient -= gram[:, j] * (old_w_j - w[j])\n",
    "        # END TODO\n",
    "\n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "         <li>Compute a precise minimum with your favorite solver</li>\n",
    "         <li>Compare the performance of cyclic and greedy CD as function of iterations.</li>\n",
    "         <li>From a practical point of view, could you use greedy CD for L2 regularized logistic regression? to solve OLS, but with 100,000 features? Explain your answers.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will do the plots using the number of iterations on the x-axis and not time as your code is likely to be slow unless you use [numba](https://numba.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a precise minimum with scipy.optimize.fmin_l_bfgs_b\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def loss(w):\n",
    "    return norm(A.dot(w) - b) ** 2 / 2.\n",
    "\n",
    "def grad(w):\n",
    "    return A.T.dot(A.dot(w) - b)\n",
    "\n",
    "w_min, _, _ = fmin_l_bfgs_b(loss, np.zeros(n_features), grad, pgtol=1e-30, factr=1e-30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (3549423389.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5938/3549423389.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    plt.semilogy((all_objs[name] - w_min)).clip(1e-30), label=name)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of cyclic and greedy CD as function of iterations with semylogy\n",
    "\n",
    "n_iter = 10000\n",
    "all_objs = {}\n",
    "for name, method in [(\"cyclic\", cyclic_cd), (\"greedy\", greedy_cd)]:\n",
    "    _, all_objs[name] = method(A, b, n_iter)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for name in all_objs:\n",
    "    plt.semilogy((all_objs[name] - w_min)).clip(1e-30), label=name)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Convergence plot for cyclic and greedy CD\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b><br/>\n",
    "    Assuming uniqueness of the solution, show that: $\\lambda \\geq \\lambda_{max} \\Leftrightarrow \\hat w = 0$\n",
    "where $\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}$.\n",
    "</div>\n",
    "\n",
    "**HINT:** You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\n",
    "\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert (w_p) $$\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and $$ \\partial \\vert \\cdot \\vert (w_j) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b><br/>\n",
    "    Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Adjusted answer starts here\n",
    "\n",
    "By a theorem, we can establish that a twice-differentiable function, which is \\(L\\)-smooth, is bounded by \\(L\\):\n",
    "\n",
    "\\[\n",
    "\\text{F is } L\\text{-Smooth and twice-differentiable} \\iff \\|\\nabla^2F(w)\\|_2 \\leq L\n",
    "\\]\n",
    "\n",
    "Now, let's compute the second derivatives:\n",
    "\n",
    "Let \\(\\sigma(x) = \\frac{1}{1+\\mathrm{e}^{-x}}\\) for simplification.\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial^2}{\\partial w_j^2} F(w_j) &= \\sum_{i=1}^n\\dfrac{\\partial^2}{\\partial w_j^2}\\log \\big( 1 + e^{- y_i w^\\top x_i} \\big) \\\\\n",
    "&= \\sum_{i=1}^n \\dfrac{\\partial}{\\partial w_j} \\Big(-y_i x_{i,j}\\big(\\sigma(- y_i w^\\top x_{i}) \\big) \\Big) \\\\\n",
    "&= \\sum_{i=1}^n (-y_i x_{i,j})^2\\sigma'(y_i w^\\top x_{i})\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "We know that \\(\\sigma'(x) = \\sigma(x)\\big(1-\\sigma(x)\\big) \\leq \\frac{1}{4}\\). \n",
    "\n",
    "Moreover, for logistic regression, we have \\(y_i \\in \\{-1, +1\\}\\) and \\(y_i^2 = 1\\).\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial^2}{\\partial w_j^2} F(w_j) &\\leq \\frac{1}{4} \\sum_{i=1}^n (y_i x_{i,j})^2 \\\\\n",
    "&\\leq \\frac{1}{4} \\sum_{i=1}^n x^2_{i,j} \\\\\n",
    "&\\leq \\frac{1}{4} \\|X_j\\|_2^2 \\quad \\text{with } X_j \\in \\mathbb{R}^p\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "So, we can express:\n",
    "\n",
    "\\[\n",
    "\\boxed{\\gamma_j = L_j = \\frac{1}{4} \\|X_j\\|_2^2}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b><br/>\n",
    "    Code cyclic proximal coordinate descent for sparse Logistic regression:\n",
    "</div>\n",
    "\n",
    "**WARNING**: the Lasso means linear regression (quadratic fitting term) with L1 penalty. Sparse logistic regression means logistic regression with L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 20.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.sign(x) * np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(X, axis=0) ** 2 / 4.\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "            grad_j = -y * sigmoid(-y * Xw)\n",
    "            w[j] = soft_thresh(1, 2)\n",
    "            \n",
    "            if old_w_j != w[j]:\n",
    "                Xw += X[:, j] * (old_w_j - w[j])\n",
    "            #END TODO\n",
    "            \n",
    "        all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/lib/python3.9/site-packages/sklearn/datasets/_openml.py:417: UserWarning: Multiple active versions of the dataset matching the name leukemia exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "leuk = fetch_openml(\"leukemia\")\n",
    "\n",
    "X = np.asfortranarray(leuk.data)\n",
    "y = np.ones(leuk.target.shape)\n",
    "y[leuk.target == leuk.target[0]] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7129)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "lambda_max_lasso = norm(X.T.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6:</b> Code\n",
    "    <ul>\n",
    "        <li>a simple proximal gradient solver for the Lasso</li>\n",
    "        <li>a prox CD solver for the Lasso and compare them on this dataset.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_linreg(w, X, y, lamb):\n",
    "    return X.T.dot(X.dot(w) - y)\n",
    "\n",
    "def loss_linreg(w, X, y, lamb):\n",
    "    return np.linalg.norm(X @ w - y) ** 2 + lamb * norm(w, ord=1)\n",
    "\n",
    "def PGD(X, y, loss_f, grad_f, lbd, n_iter):\n",
    "    \"\"\"Proximal gradient descent algorithm\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randint(n_features) * 0.01\n",
    "    objectives = []\n",
    "    step = n_samples / ((norm(X, ord=2)**2))\n",
    "\n",
    "    objectives.append(loss_f(w, X, y, lbd))\n",
    "    for k in range(n_iter + 1):\n",
    "        w = soft_thresh(w - step * grad_f(w, X, y, lbd), lbd * step)\n",
    "        # Current objective\n",
    "        objectives.append(loss_f(w, X, y, lbd))\n",
    "\n",
    "    return w, np.array(objectives)\n",
    "\n",
    "def cd_linreg(X, y, loss_f, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.random.randint(n_features) * 0.01\n",
    "    Xw = X.dot(w)\n",
    "    grad = X.T @ (Xw - y)\n",
    "    # TODO\n",
    "    step =  n_samples / [norm(X[:, j], ord=2)**2 for j in range(n_features)]\n",
    "    gram = X.T @ X\n",
    "    all_objs = np.zeros(n_iter+1)\n",
    "    all_objs[0] = loss_f(w, X, y, lamb)\n",
    "    # END TODO\n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            grad_j = grad[j]\n",
    "            w[j] = soft_thresh(old_w_j - grad_j * step[j], lamb * step[j])\n",
    "\n",
    "            if old_w_j != w[j]:\n",
    "                grad += (w[j] - old_w_j) * gram[:,j]\n",
    "                Xw += X[:, j] * (w[j] - old_w_j) \n",
    "                \n",
    "        all_objs[t+1] = loss_f(w, X, y, lamb)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5938/2122112310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# PGD Logistic Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpgd_x_leuk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_x_list_leuk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_linreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_linreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Proximal Coordinate Descent Logistic Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproximal_x_leuk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproximal_x_list_leuk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcd_linreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_linreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5938/326543953.py\u001b[0m in \u001b[0;36mPGD\u001b[0;34m(X, y, loss_f, grad_f, lbd, n_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5938/326543953.py\u001b[0m in \u001b[0;36mloss_linreg\u001b[0;34m(w, X, y, lamb)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_linreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "n_iter = 15000\n",
    "# PGD Logistic Regression\n",
    "pgd_x_leuk, pgd_x_list_leuk = PGD(X, y, loss_linreg, grad_linreg, lbd=lambd, n_iter=n_iter)\n",
    "# Proximal Coordinate Descent Logistic Regression\n",
    "proximal_x_leuk, proximal_x_list_leuk = cd_linreg(X, y, loss_linreg, lambd, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
